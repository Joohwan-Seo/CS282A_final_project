{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEbjxXTLL44D"
      },
      "source": [
        "# Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCWPmF5DL8_5"
      },
      "source": [
        "Mount your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAY0-sMo9tAR",
        "outputId": "6620f5c2-d979-425c-8a3e-b192c80e4b3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrtWHnYbMBj2"
      },
      "source": [
        "Set up mount symlink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r5kJBJ60-10F"
      },
      "outputs": [],
      "source": [
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/PointNet'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "## the space in `My Drive` causes some issues,\n",
        "## make a symlink to avoid this\n",
        "SYM_PATH = '/content/PointNet'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZNpfeuJMFQg"
      },
      "source": [
        "## Import and others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "qVwSTMaw_jzB"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import  matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXPMM9KtDwPj",
        "outputId": "21122a34-a3eb-4c2d-960b-a20b98fa9435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dg_Ok4y5qZr",
        "outputId": "d5192e96-d11e-4f5d-b508-41e64c0c9f62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM4FYhadbfGG"
      },
      "source": [
        "## Download data\n",
        "Original dataset takes too much time in reading files and sampling - so we did it beforehand.\n",
        "\n",
        "It reduced training time from 40 mins per epoch to 1 mins per epoch!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msAfpLclcuUv",
        "outputId": "ccc08b24-b0f8-4a28-ca0b-bc3dc83aa6dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/PointNet\n",
            "Cloning into 'PointCloudAlchemist'...\n",
            "remote: Enumerating objects: 70, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 70 (delta 23), reused 30 (delta 13), pack-reused 23\u001b[K\n",
            "Unpacking objects: 100% (70/70), 232.13 MiB | 7.06 MiB/s, done.\n",
            "Updating files: 100% (22/22), done.\n",
            "/content/gdrive/My Drive/PointNet/PointCloudAlchemist\n"
          ]
        }
      ],
      "source": [
        "%cd $SYM_PATH\n",
        "\n",
        "# This may take a few minutes\n",
        "if not os.path.exists(\"PointCloudAlchemist\"):\n",
        "    !git clone https://github.com/Joohwan-Seo/CS282A_final_project.git PointCloudAlchemist\n",
        "else:\n",
        "    print(\"Already downloaded.\")\n",
        "\n",
        "%cd PointCloudAlchemist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmH16Hst2jZJ",
        "outputId": "f9582bb3-0cd1-42bf-e440-6e874aa5efa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/PointNet/PointCloudAlchemist\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-BlYpOHsaXBi"
      },
      "outputs": [],
      "source": [
        "from utils import PointCloudData, Normalize, RotateXYZ, AddGaussianNoise, read_off, view_mesh, view_scatter\n",
        "from network_sol import PointNetClassification, PointNetSegmentation, GetModel, PointNetLoss\n",
        "from train_sol import TrainModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNRmobM2MPLD"
      },
      "source": [
        "## visualize image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_AOP0X6TLT7"
      },
      "source": [
        "The orignal dataset has objects given in a mesh form. Here is a visualisation of a chair in the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EbXarR_0TLT7"
      },
      "outputs": [],
      "source": [
        "verts, faces = read_off(\"data_for_viz/chair_0003.off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tjkYvOOZTLT7"
      },
      "outputs": [],
      "source": [
        "i,j,k = np.array(faces).T\n",
        "x,y,z = np.array(verts).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUYEZzwyTLT7"
      },
      "outputs": [],
      "source": [
        "view_mesh(x,y,z, i,j,k )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K-fCUYkTLT7"
      },
      "source": [
        "Now we turn the mesh objects into a point cloud using three different sampling methods. We can sample points randomly from the mesh faces, here is what that point cloud looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV7t2yrsTLT7"
      },
      "outputs": [],
      "source": [
        "default = np.load(\"data_for_viz/chair_default.pkl\", allow_pickle=True)\n",
        "view_scatter(*default)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMMpJYB2TLT7"
      },
      "source": [
        "Here is the result of the original sampling method as proposed by the paper, where points are sampled from the faces of the objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoyyQdlaTLT8"
      },
      "outputs": [],
      "source": [
        "original = np.load(\"data_for_viz/chair_original.pkl\", allow_pickle=True)\n",
        "view_scatter(*original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJD010iCTLT8"
      },
      "source": [
        "Here is the point cloud from sampling based on the farthest point, where we find the centroid of the mesh surface and sample the farthest point from it, we iterate this to get the following point cloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs0vSsy6TLT8"
      },
      "outputs": [],
      "source": [
        "farthest = np.load(\"data_for_viz/chair_farthest.pkl\", allow_pickle=True)\n",
        "view_scatter(*farthest)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj0bE6YrMfca"
      },
      "source": [
        "### Implement transforms function \n",
        "We first define the data transforms, which is called by `torch.DataLoader`.\n",
        "The transforms normalize, rotate and add noise to the pointclouds.\\\n",
        "Hint: look into the functions in utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "T8hG6GT-MxTH"
      },
      "outputs": [],
      "source": [
        "#TODO Apply transform for the torch.DataLoader\n",
        "#Hint: look into the functions in utils.py\n",
        "pointnet_tf = transforms.Compose([#arg1,\n",
        "                                  #arg2,\n",
        "                                  #arg3,\n",
        "                                  transforms.ToTensor()\n",
        "                                ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JpC5qdg9XVh2"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(340420)\n",
        "\n",
        "method = 'random'\n",
        "\n",
        "all_data = {'train': PointCloudData(data_split = \"train\", method = method, acc_type = 'valid' , transform = pointnet_tf),\n",
        "            'test' : PointCloudData(data_split = \"test\",  method = method, acc_type = 'valid' , transform = pointnet_tf)\n",
        "            } \n",
        "\n",
        "all_dataloader = {'train': DataLoader(dataset = all_data['train'], batch_size = 32, shuffle = True),\n",
        "                  'test' : DataLoader(dataset = all_data['test'] , batch_size = 64)\n",
        "                 }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXCUaglmZ58Q"
      },
      "source": [
        "The details of the dataset can be found in the following block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SV6zhlDMnGp",
        "outputId": "35bbc998-ea63-4669-a92b-130d4b878eb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of train data = 3991\n",
            "# of test data = 908\n",
            "# of classes: 10\n",
            "Sampled pointcloud shape: torch.Size([1024, 3])\n",
            "Class of the 3990-th train data = toilet\n"
          ]
        }
      ],
      "source": [
        "num_to_class = {i: cat for cat, i in all_data['train'].classes.items()}\n",
        "\n",
        "for t in ['train', 'test']:\n",
        "    print(f\"# of {t} data = {len(all_data[t])}\")\n",
        "\n",
        "print(f\"# of classes: {len(all_data['train'].classes)}\")\n",
        "\n",
        "any_idx = 3990\n",
        "assert any_idx < len(all_data['train'])\n",
        "\n",
        "print(f\"Sampled pointcloud shape: {all_data['train'][any_idx]['pointcloud'].size()}\")\n",
        "print(f\"Class of the {any_idx}-th train data = {num_to_class[all_data['train'][any_idx]['category']]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Io57gyfVTQW"
      },
      "source": [
        "# Write down core functions\n",
        "Finish `network.py` and `train.py` - there are detailed comments in the code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srzb-HI9M6xA"
      },
      "source": [
        "# Model Training & Testing\n",
        "Now, we will test the training results under several conditions. \n",
        "In particular, the questions we want to answer is as follows:\n",
        "- What is the role of the Tnet?\n",
        "- How does the data sampling method affect the performance?\n",
        "\n",
        "To answer this question, we will train 4 classification models: with/without Tnet, random/farthest sampling method.\n",
        "\n",
        "For the segmentation tasks, we fix the sampling method as random, but we are only checking with with/without Tnet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3d8JC9vSYKGa"
      },
      "outputs": [],
      "source": [
        "# If you rerun this cell, you will lost all your work\n",
        "model_dict, output_dict = {}, {}\n",
        "for tnet in ['yes_t', 'no_t']:\n",
        "    model_dict[tnet], output_dict[tnet] = {}, {}\n",
        "    for task in ['Classification', 'Segmentation']:\n",
        "        model_dict[tnet][task], output_dict[tnet][task] = {}, {}\n",
        "        for method in ['random', 'farthest']:\n",
        "            model_dict[tnet][task][method], output_dict[tnet][task][method] = None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKQTQcyLNHwR"
      },
      "source": [
        "## Classification Model training\n",
        "### Case 1: With Tnet / Random\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ-ggXMfNbQ_"
      },
      "outputs": [],
      "source": [
        "tnet = 'yes_t' \n",
        "task = 'Classification'\n",
        "method = 'random'\n",
        "\n",
        "# set model and check device\n",
        "model_dict[tnet][task][method], optimizer, scheduler = GetModel(tnet, task, DEVICE)\n",
        "print(next(model_dict[tnet][task][method].parameters()).device)\n",
        "\n",
        "# train\n",
        "output_dict[tnet][task][method] = TrainModel(\n",
        "    task         = task,\n",
        "    model        = model_dict[tnet][task][method],\n",
        "    train_loader = all_dataloader['train'], # training\n",
        "    valid_loader = all_dataloader['test'],  # validation (and test)\n",
        "    num_epochs   = 30,\n",
        "    optimizer    = optimizer,\n",
        "    scheduler    = scheduler,\n",
        "    device       = DEVICE,\n",
        "    save         = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfs6ZXRkYPvY"
      },
      "source": [
        "### Case 2: Without Tnet / Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zCtLM0LMi_W"
      },
      "outputs": [],
      "source": [
        "tnet = 'no_t' \n",
        "task = 'Classification'\n",
        "method = 'random'\n",
        "\n",
        "# set model and check device\n",
        "model_dict[tnet][task][method], optimizer, scheduler = GetModel(tnet, task, DEVICE)\n",
        "print(next(model_dict[tnet][task][method].parameters()).device)\n",
        "\n",
        "# train\n",
        "output_dict[tnet][task][method] = TrainModel(\n",
        "    task         = task,\n",
        "    model        = model_dict[tnet][task][method],\n",
        "    train_loader = all_dataloader['train'], # training\n",
        "    valid_loader = all_dataloader['test'],  # validation (and test)\n",
        "    num_epochs   = 30,\n",
        "    optimizer    = optimizer,\n",
        "    scheduler    = scheduler,\n",
        "    device       = DEVICE,\n",
        "    save         = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oVXhCboYhz2"
      },
      "source": [
        "### Re-defining Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ABlhrY13Ygtv"
      },
      "outputs": [],
      "source": [
        "method = 'farthest'\n",
        "\n",
        "all_data = {'train': PointCloudData(data_split = \"train\", method = method, acc_type = 'valid' , transform = pointnet_tf),\n",
        "            'test' : PointCloudData(data_split = \"test\",  method = method, acc_type = 'valid' , transform = pointnet_tf)\n",
        "            } \n",
        "\n",
        "all_dataloader = {'train': DataLoader(dataset = all_data['train'], batch_size = 32, shuffle = True),\n",
        "                  'test' : DataLoader(dataset = all_data['test'] , batch_size = 64)\n",
        "                 }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiKK3CN-Y2Iu"
      },
      "source": [
        "## Case 3: With Tnet / Farthest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKzl7-t_YwlQ"
      },
      "outputs": [],
      "source": [
        "tnet = 'yes_t' \n",
        "task = 'Classification'\n",
        "method = 'farthest'\n",
        "\n",
        "# set model and check device\n",
        "model_dict[tnet][task][method], optimizer, scheduler = GetModel(tnet, task, DEVICE)\n",
        "print(next(model_dict[tnet][task][method].parameters()).device)\n",
        "\n",
        "# train\n",
        "output_dict[tnet][task][method] = TrainModel(\n",
        "    task         = task,\n",
        "    model        = model_dict[tnet][task][method],\n",
        "    train_loader = all_dataloader['train'], # training\n",
        "    valid_loader = all_dataloader['test'],  # validation (and test)\n",
        "    num_epochs   = 30,\n",
        "    optimizer    = optimizer,\n",
        "    scheduler    = scheduler,\n",
        "    device       = DEVICE,\n",
        "    save         = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhtlpDi_YxH7"
      },
      "source": [
        "## Case 4: Without Tnet / Farthest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heqLJM-RZBWv"
      },
      "outputs": [],
      "source": [
        "tnet = 'no_t' \n",
        "task = 'Classification'\n",
        "method = 'farthest'\n",
        "\n",
        "# set model and check device\n",
        "model_dict[tnet][task][method], optimizer, scheduler = GetModel(tnet, task, DEVICE)\n",
        "print(next(model_dict[tnet][task][method].parameters()).device)\n",
        "\n",
        "# train\n",
        "output_dict[tnet][task][method] = TrainModel(\n",
        "    task         = task,\n",
        "    model        = model_dict[tnet][task][method],\n",
        "    train_loader = all_dataloader['train'], # training\n",
        "    valid_loader = all_dataloader['test'],  # validation (and test)\n",
        "    num_epochs   = 30,\n",
        "    optimizer    = optimizer,\n",
        "    scheduler    = scheduler,\n",
        "    device       = DEVICE,\n",
        "    save         = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl2yFXHRZAsV"
      },
      "source": [
        "## Part segmentation models training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORZuxxpDaK17"
      },
      "source": [
        "Re-defining dataloader for the Segmentation tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "U0A-auZVn_Lv"
      },
      "outputs": [],
      "source": [
        "all_data = {'train': PointCloudData(data_split = \"train\", method = 'random', task = 'Segmentation', acc_type = 'valid' , transform = pointnet_tf),\n",
        "            'test' : PointCloudData(data_split = \"test\",  method = 'random', task = 'Segmentation', acc_type = 'valid' , transform = pointnet_tf)\n",
        "            } \n",
        "all_dataloader = {'train': DataLoader(dataset = all_data['train'], batch_size = 32, shuffle = True),\n",
        "                  'test' : DataLoader(dataset = all_data['test'] , batch_size = 64)\n",
        "                 }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIAM_39vaR2a"
      },
      "source": [
        "## Case 1: With Tnet / Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r_VGm8tWxSN"
      },
      "outputs": [],
      "source": [
        "tnet = 'yes_t' \n",
        "task = 'Segmentation'\n",
        "method = 'random'\n",
        "\n",
        "# set model and check device\n",
        "model_dict[tnet][task][method], optimizer, scheduler = GetModel(tnet, task, DEVICE)\n",
        "print(next(model_dict[tnet][task][method].parameters()).device)\n",
        "\n",
        "# train\n",
        "output_dict[tnet][task][method] = TrainModel(\n",
        "    task         = task,\n",
        "    model        = model_dict[tnet][task][method],\n",
        "    train_loader = all_dataloader['train'], # training\n",
        "    valid_loader = all_dataloader['test'],  # validation (and test)\n",
        "    num_epochs   = 30,\n",
        "    optimizer    = optimizer,\n",
        "    scheduler    = scheduler,\n",
        "    device       = DEVICE,\n",
        "    save         = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mOghvI7admE"
      },
      "source": [
        "## Case 2: With Tnet / Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-8hVOUyWxLo"
      },
      "outputs": [],
      "source": [
        "tnet = 'no_t' \n",
        "task = 'Segmentation'\n",
        "method = 'random'\n",
        "\n",
        "# set model and check device\n",
        "model_dict[tnet][task][method], optimizer, scheduler = GetModel(tnet, task, DEVICE)\n",
        "print(next(model_dict[tnet][task][method].parameters()).device)\n",
        "\n",
        "# train\n",
        "output_dict[tnet][task][method] = TrainModel(\n",
        "    task         = task,\n",
        "    model        = model_dict[tnet][task][method],\n",
        "    train_loader = all_dataloader['train'], # training\n",
        "    valid_loader = all_dataloader['test'],  # validation (and test)\n",
        "    num_epochs   = 30,\n",
        "    optimizer    = optimizer,\n",
        "    scheduler    = scheduler,\n",
        "    device       = DEVICE,\n",
        "    save         = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuU_B1RPahGV"
      },
      "source": [
        "# Plotting for the training loss and validation accuracy\n",
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "F_GwSbzhaiq_",
        "outputId": "d72d65dd-7595-4127-8059-f96d4c864b81"
      },
      "outputs": [],
      "source": [
        "arg_tnet = ['yes_t', 'no_t']\n",
        "arg_method = ['farthest', 'random']\n",
        "\n",
        "# For the classification task\n",
        "task = 'Classification'\n",
        "k = 1\n",
        "plt.figure(1, figsize = (12,8))\n",
        "legend = []\n",
        "for tnet in arg_tnet:\n",
        "  for method in arg_method:\n",
        "    plt.subplot(2,1,1)\n",
        "    data = output_dict[tnet][task][method]\n",
        "    train_loss = []\n",
        "    acc = []\n",
        "    N = len(data)\n",
        "    for i in range(N):\n",
        "      train_loss.append(np.mean(data[i]['train_loss']))\n",
        "      acc.append(data[i]['accuracy'])\n",
        "\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(range(N), train_loss)\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(range(N), acc)\n",
        "    legend.append(tnet + ',' + method)\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.legend(legend);\n",
        "plt.ylabel('Training Loss');\n",
        "plt.subplot(2,1,2)\n",
        "plt.legend(legend);\n",
        "plt.ylabel('Validation Accuracy');\n",
        "plt.xlabel('num epochs');\n",
        "\n",
        "    # break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFdo1tdKfbW8"
      },
      "source": [
        "## Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UgCZ4u8azj5"
      },
      "outputs": [],
      "source": [
        "arg_tnet = ['yes_t', 'no_t']\n",
        "arg_method = ['random']\n",
        "\n",
        "# For the classification task\n",
        "task = 'Segmentation'\n",
        "k = 1\n",
        "plt.figure(1, figsize = (12,8))\n",
        "legend = []\n",
        "for tnet in arg_tnet:\n",
        "  for method in arg_method:\n",
        "    plt.subplot(2,1,1)\n",
        "    data = output_dict[tnet][task][method]\n",
        "    train_loss = []\n",
        "    acc = []\n",
        "    N = len(data)\n",
        "    for i in range(N):\n",
        "      train_loss.append(np.mean(data[i]['train_loss']))\n",
        "      acc.append(data[i]['accuracy'])\n",
        "\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(range(N), train_loss)\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(range(N), acc)\n",
        "    legend.append(tnet + ',' + method)\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.legend(legend);\n",
        "plt.ylabel('Training Loss');\n",
        "plt.subplot(2,1,2)\n",
        "plt.legend(legend);\n",
        "plt.ylabel('Validation Accuracy');\n",
        "plt.xlabel('num epochs');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYlN64WIgkN0"
      },
      "source": [
        "# Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "L--N-ZjWgkEC"
      },
      "outputs": [],
      "source": [
        "tnet = 'yes_t' \n",
        "task = 'Segmentation'\n",
        "method = 'random'\n",
        "\n",
        "mymodel = model_dict[tnet][task][method]\n",
        "mymodel.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB5z1QANfjbV",
        "outputId": "4a77fcba-54fd-4f7b-a9c5-670067b0c218"
      },
      "outputs": [],
      "source": [
        "test_data = PointCloudData(data_split = \"test\",  method = method, task = task, acc_type = 'test' , transform = None)\n",
        "test_loader = DataLoader(dataset = test_data , batch_size = 64)\n",
        "\n",
        "with torch.no_grad():\n",
        "  correct, total = 0, 0\n",
        "  for data in test_loader:\n",
        "      # get pointcloud and category\n",
        "      pointcloud = data['pointcloud'].to(DEVICE).float()\n",
        "      category   = data['category'].to(DEVICE)\n",
        "      # run the model\n",
        "      output, _, _ = mymodel(pointcloud.transpose(1,2))\n",
        "      _, predict_category = torch.max(output.data, 1)\n",
        "      # record the correct predicitons\n",
        "      if task == \"Classification\":\n",
        "        total += predict_category.size(0)\n",
        "      elif task == \"Segmentation\":\n",
        "        total += predict_category.size(0) * predict_category.size(1)  \n",
        "      correct += (predict_category == category).sum().item()\n",
        "\n",
        "accuracy = int((correct / total) * 100)\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRs_GZ9VhoYJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
